{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle DeepFake Competition <a name=\"start\">\n",
    "In-depth review of the winner's code - prediction part only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to the competition : httmps://www.kaggle.com/c/deepfake-detection-challenge\n",
    "\n",
    "Link to the Selim Seferbekov's winning repo : https://github.com/selimsef/dfdc_deepfake_challenge"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Commentaries starting with ## and pydoc are the original commentaries from the author.\n",
    "All the other commentaries in this notebook are my description, almost line by line, of the author's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Model preparation ](#model)\n",
    "    1. [Efficient net](#efficient)\n",
    "    2. [Adding custom layers to the efficient net](#custom)\n",
    "3. [Data preparation](#data)\n",
    "    1. [Video frame extraction](#frame)\n",
    "    2. [Face detection](#detect)\n",
    "    3. [Intermediate scoring strategy](#scoring)\n",
    "    4. [Image manipulation](#manip)\n",
    "    5. [Data preparation's main pipeline](#pipeline)\n",
    "4. [Main porcessing loop](#loop)\n",
    "    1. [Per file processing](#process)\n",
    "    2. [Small parenthesis](#parenthesis)\n",
    "    3. [Aside thought](#aside)\n",
    "    4. [Prediction](#pred)\n",
    "5. [Subsidiary questions](#subs)\n",
    "    1. [What does partial do ?](#partial)\n",
    "    2. [Numpy linspace](#linspace)\n",
    "    3. [Numpy Rand randint](#randint)\n",
    "    4. [Numpy clip](#clip)\n",
    "    5. [Numpy uint8 format](#uint8)\n",
    "    6. [Numpy count_nonzero](#zero)\n",
    "    7. [Calling float() or putting the argument dtype=torch.float32](#float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports <a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial                               # Way to partially call a function and \n",
    "                                                            # keep its intermediary stage.\n",
    "import os                                                   # File system navigation.\n",
    "import sys                                                  # System information access.\n",
    "import time                                                 # Time measure tools.\n",
    "import traceback                                            # Print the stacktrace when an exception is manually caught.\n",
    "\n",
    "import torch\n",
    "from timm.models.efficientnet import tf_efficientnet_b7_ns  # We will only use the b7 ns version.\n",
    "from torch.nn.modules.pooling import AdaptiveAvgPool2d      # AVG Pooling2D filter.\n",
    "from torch.nn.modules.dropout import Dropout                # Dropout procedure.\n",
    "from torch.nn.modules.linear import Linear                  # FullyConnected Neural Network.\n",
    "\n",
    "from torchvision.transforms import Normalize                # Image normalization.\n",
    "from facenet_pytorch.models.mtcnn import MTCNN              # Face Detection model.\n",
    "\n",
    "from PIL import Image                                       # Image manipulation library with its own class Image.\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor           # Using several workers to process parrallelizable tasks.\n",
    "\n",
    "import cv2                                                  # OpenCv for many kinds of image manipulation\n",
    "import numpy as np                                          # Mathematical arrays.\n",
    "import pandas as pd                                         # DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please put the train and test video folders' relative or absolute paths in these variables :\n",
    "train_video_folder = \"N:/Datasets/DeepFake_Kaggle/train_sample_videos\"\n",
    "test_video_folder = \"N:/Datasets/DeepFake_Kaggle/test_videos\"\n",
    "# Let's put it true to see some steps information at the execution.\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation  <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient net   <a name=\"efficient\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we only keep the best model but want to reproduce the implementation of all the necessary code to predict, we will keep the best model's params inside the encoder_params dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's only keep the params that were ultimately used\n",
    "encoder_params = {\"tf_efficientnet_b7_ns\": \n",
    "           {\"features\": 2560,\n",
    "              \"init_op\": partial(tf_efficientnet_b7_ns, pretrained=True, drop_path_rate=0.2)}\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding custom layers to the efficient net <a name=\"custom\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get the face images into a EfficientNet before passing it to a last filtering step \n",
    "# and a Fully COnnected Neural NEtwork for the final prediction.\n",
    "class DeepFakeClassifier(torch.nn.Module):\n",
    "    def __init__(self, encoder, dropout_rate=0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
    "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.fc = Linear(encoder_params[encoder][\"features\"], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We encode the image using our EfficientNet. Please refer to the readme.\n",
    "        x = self.encoder.forward_features(x)\n",
    "        # We pass the resulting data to an Average Pooling convolution.\n",
    "        # flatten(x) changes the size so that there all dimensions from index x are downsized to one.\n",
    "        # flatten(1) means that the result will have 2 dimensions (the second wil contain all the other dimension values)\n",
    "        x = self.avg_pool(x).flatten(1)\n",
    "        # Explained in the readme. The dropout is turned off by the call to model.eval(). \n",
    "        # It is turned on by model.train()\n",
    "        x = self.dropout(x)\n",
    "        # Our Linear (or dense or fully connected) layer, with the count of features in input size and 1 output neuron,\n",
    "        # which will give us the probability of the image being fake. A value equal or greater than 0.5 \n",
    "        # will utltimately mean fake.\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a CUDA compatible GPU boosts the speed of all the previously implemented processing.\n",
    "model = DeepFakeClassifier(encoder=\"tf_efficientnet_b7_ns\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I could not find the weights (error 404 from the authors's Git) and I clearly don't have time to train the network. That's why we will focus on the prediction step in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation  <a name=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video frame extraction <a name=\"frame\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes an image and apply a color conversion, and potentially some cropping.\n",
    "def _post_process_frame(frame):\n",
    "    # First, it converts opencv's Blue-Green-Red format to the more conventional Red-Green-Blue.\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # In the original repo, we have cropping according to specified insets here.\n",
    "    # But it is practically always called with default values which are (0, 0) (no crop).\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the video, extract the frames at specified indices,\n",
    "# apply post_processing on it and return them all as a list of frame.\n",
    "def read_frames_at_indices(path, frame_idxs):\n",
    "    \"\"\"Reads frames from a video and puts them into a NumPy array.\n",
    "\n",
    "    Arguments:\n",
    "        path: the video file\n",
    "        frame_idxs: a list of frame indices. Important: should be\n",
    "            sorted from low-to-high! If an index appears multiple\n",
    "            times, the frame is still read only once.\n",
    "\n",
    "    Returns:\n",
    "        - a NumPy array of shape (num_frames, height, width, 3)\n",
    "        - a list of the frame indices that were read\n",
    "\n",
    "    Reading stops if loading a frame fails, in which case the first\n",
    "    dimension returned may actually be less than num_frames.\n",
    "\n",
    "    Returns None if an exception is thrown for any reason, or if no\n",
    "    frames were read.\n",
    "    \"\"\"\n",
    "    assert len(frame_idxs) > 0\n",
    "    # Loads the video capture.\n",
    "    capture = cv2.VideoCapture(path)\n",
    "    # The try catch will make sure we release the capture, after an exception or not.\n",
    "    # Releasing the capture is necessary to release the hardware and software resources,\n",
    "    # so it is ready for the next video.\n",
    "    try:\n",
    "        # frames will store the kept frames.\n",
    "        # idxs_read will store their indices.\n",
    "        frames = []\n",
    "        idxs_read = []\n",
    "        for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
    "            ## Get the next frame, but don't decode if we're not using it.\n",
    "            # The grab method is faster than \"read\" as it doesn't decode it for further processing.\n",
    "            # Indeed, we either process it or completely pass it, \n",
    "            # therefore it's more optimized to use grab to pass.\n",
    "            ret = capture.grab()\n",
    "            # We check if the frame was correctly grabbed.\n",
    "            # If not, it will exit the for loop and return None.\n",
    "            if not ret:\n",
    "                if verbose:\n",
    "                    print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n",
    "                break\n",
    "\n",
    "            ## Need to look at this frame?\n",
    "            # The next frame to retrieve and memorize. \n",
    "            # For example, if 7 frames were already retrieved, then the next frame to retrieve will be in our\n",
    "            # frame_idx argument at position 7.\n",
    "            current = len(idxs_read)\n",
    "            # If the current frame is effectively the next one we are looking for, we can retrieve it.\n",
    "            if frame_idx == frame_idxs[current]:\n",
    "                # The retrieve method returns a tuple with:\n",
    "                # - a bool to tell if the image was correctly retrieved.\n",
    "                # - the retrieved image itself.\n",
    "                ret, frame = capture.retrieve()\n",
    "                if not ret or frame is None:\n",
    "                    if verbose:\n",
    "                        print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
    "                    break\n",
    "                # We have to convert opencv's BGR color format to RGB.\n",
    "                frame = _post_process_frame(frame)\n",
    "                # We keep this frame in the list of selected frames.\n",
    "                frames.append(frame)\n",
    "                # Also, we keep its index in the other separated list.\n",
    "                idxs_read.append(frame_idx)\n",
    "            \n",
    "        # Now, we can stack our frames. Stack will simply transform a list of multidimensional arrays into\n",
    "        # an array of multidimensional arrays. It does not imply dimension loss.\n",
    "        if len(frames) > 0:\n",
    "            return np.stack(frames), idxs_read\n",
    "        if verbose:\n",
    "            print(\"No frames read from movie %s\" % path)\n",
    "        return None\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"Exception while reading movie %s\" % path)\n",
    "            traceback.print_exc() \n",
    "        return None\n",
    "    finally:\n",
    "        # Thanks to the finally clause, we release the capture \n",
    "        # even if an exception was caught and after a \"return\".\n",
    "        capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects the index of evenlly separated frames, passes it to the previsously implemented\n",
    "# read_frames_at_indices() function, so it can return the associated frames in the wanted RGB format.\n",
    "def read_frames(path, num_frames, jitter=0, seed=None):\n",
    "    \"\"\"Reads frames that are always evenly spaced throughout the video.\n",
    "\n",
    "    Arguments:\n",
    "        path: the video file\n",
    "        num_frames: how many frames to read, -1 means the entire video\n",
    "            (warning: this will take up a lot of memory!)\n",
    "        jitter: if not 0, adds small random offsets to the frame indices;\n",
    "            this is useful so we don't always land on even or odd frames\n",
    "        seed: random seed for jittering; if you set this to a fixed value,\n",
    "            you probably want to set it only on the first video\n",
    "    \"\"\" \n",
    "    assert num_frames > 0\n",
    "    # Simply loads the video capture, using OpenCV.\n",
    "    capture = cv2.VideoCapture(path) \n",
    "    # Gets the total number of frame of the video\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # If the video is \"empty\", simply returns None\n",
    "    if frame_count <= 0:\n",
    "        return None\n",
    "    \n",
    "    # This would get us [frame_counts] frames equally spaced on a linear scale of all the frames in the video.\n",
    "    # And we cant the first and last frames in it.\n",
    "    frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=np.int)\n",
    "    # We don't want to take only odd or even frames, so we add random jittering, reproducible thanks to the seed.\n",
    "    if jitter > 0:\n",
    "        # We initialize the seed of the pseudo-random generator\n",
    "        np.random.seed(seed)\n",
    "        # For each selected frame index, we want to add or substract the value of the  jitter argument.\n",
    "        jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
    "        # Next we add the jitters to each frame index and we pass the result to a clip function,\n",
    "        # to make sure that we won't keep an out of bounds index.\n",
    "        # ?BEWARE? it might take the same indices several times, especially the\n",
    "        # boundaries indices, or when the total number of frames is low.\n",
    "        # If we had added more jitter, we should have found a better way to make sure we don't.\n",
    "        frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
    "    # Now we can finally get the frames of all the selected indices.\n",
    "    result = read_frames_at_indices(path, frame_idxs)\n",
    "    # As always, we have to close the capture to release all induced resources.\n",
    "    capture.release()\n",
    "    return result\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection <a name=\"detect\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is  ready, as weel as our video image exctracting. Now we need to extract the bounding boxes of the faces, thanks to a specific MTCNN network called Facenet. (Please refer to the readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is aimed to extract all the faces, cropped, from a selection of video frames.\n",
    "# The cropping of the faces is done thanks to the facenet neural network.\n",
    "# The frame index selection is done thanks to the function we just made and passed as argument in a partial state.\n",
    "# Please refer to the section \"Subsidiary questions\" to know more about the partial method.\n",
    "def extract_faces(video_read_fn, video_path):\n",
    "    # Lists to store infos and data about each face, as dictionaries.\n",
    "    results = []\n",
    "\n",
    "    # Our MTCNN. Its process will use the available CUDA device.\n",
    "    detector = MTCNN(margin=0, thresholds=[0.7, 0.8, 0.8], device=\"cuda\")\n",
    "\n",
    "    # Calls the function with the video_path as argument\n",
    "    result = video_read_fn(video_path)\n",
    "    if result is None:\n",
    "        return None\n",
    "    ## Keep track of the original frames (need them later).\n",
    "    my_frames, my_idxs = result\n",
    "    \n",
    "    for i, frame in enumerate(my_frames):\n",
    "        # Our video frames have 3 dimensions, we need to get the length \n",
    "        # of the first and second to get their height and width.\n",
    "        # The third is the colors channels.\n",
    "        h, w = frame.shape[:2]\n",
    "        # np.uint8 is a 8 bit integer which takes only one byte and can store integer values from 0 to 255.\n",
    "        # This cast into 8 bytes integer makes it an array that is directly transcriptable to a pillow Image.\n",
    "        img = Image.fromarray(frame.astype(np.uint8))\n",
    "        # We can now resize it by half height and half width\n",
    "        image = img.resize(size=[s // 2 for s in img.size])\n",
    "        # Example : We started the loop with a frame of 768*1024*3 values as int64\n",
    "        # we now have a frame of 384*512*3 values as uint8.\n",
    "\n",
    "        # Uses our facenet's detect method to detect faces. \n",
    "        # We only want the bounding boxes of the faces, not their landmarks (like mouth, eyes and nose).\n",
    "        # probs is the confidence the model gives to each box showing a face.\n",
    "        batch_boxes, probs = detector.detect(img, landmarks=False)\n",
    "\n",
    "        if batch_boxes is None:\n",
    "            # No faces were detected\n",
    "            return None\n",
    "\n",
    "        faces = []\n",
    "        scores = []\n",
    "\n",
    "        for bbox, score in zip(batch_boxes, probs):\n",
    "            # bbox is a tuple of 4 integer :\n",
    "            # 1- x-coord of the left edge\n",
    "            # 2- y-coord of the bottom edge\n",
    "            # 3- x-coord of the right edge\n",
    "            # 4- y-coord of the top edge\n",
    "            if bbox is not None:\n",
    "                # Since we divided the size of our original image by 2,\n",
    "                # we have to multiply the box coordinates by 2 to retrieve\n",
    "                # the predicted face locations from the original image.\n",
    "                xmin, ymin, xmax, ymax = [int(b * 2) for b in bbox]\n",
    "                # width of the box = left edge's x - right edge's x\n",
    "                w = xmax - xmin\n",
    "                # height of the box = top edge's y - bottom edge's y\n",
    "                h = ymax - ymin\n",
    "                # We are going to keep only a third of those dimension sizes for incoming selection.\n",
    "                p_h = h // 3\n",
    "                p_w = w // 3\n",
    "                # Our frame being a numpy array, we can slice it according to two dimensions ;\n",
    "                # this action actually produce a cropping.\n",
    "                # We are actually going to crop a larger box than the detected face bounding boxes.\n",
    "                # the max(ymin - p_h, 0) insures we don't try to slice with negative row index,\n",
    "                # which would give us an error. We do the same with columns index.\n",
    "                # Oddly, the author didn't do the 'min' warranty of the top row index and right column index.\n",
    "                crop = frame[max(ymin - p_h, 0):ymax + p_h, max(xmin - p_w, 0):xmax + p_w]\n",
    "                # We save this cropped image of a face in the faces list. The crop comes from the original image, \n",
    "                # not the smaller one we gave to the facenet, because we want the better definition.\n",
    "                faces.append(crop)\n",
    "                scores.append(score)\n",
    "\n",
    "        # We keep the video reference, the face image, some meta infos, the confidence score and the face image\n",
    "        # in a convenient dictionary.\n",
    "        frame_dict = {\"video\": video_path,\n",
    "                      \"frame_idx\": my_idxs[i],\n",
    "                      \"frame_w\": w,\n",
    "                      \"frame_h\": h,\n",
    "                      \"faces\": faces,\n",
    "                      \"scores\": scores}\n",
    "        # We append the result list with this dictionary, insuring we have the same format for all videos.\n",
    "        results.append(frame_dict)\n",
    "    # We now have a list of all face images from our video.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate scoring strategy <a name=\"scoring\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function highers the confidence for fakeness above a fixed threshold,\n",
    "# and also higher the confidence for non-fakeness below a certain percentage of low value.\n",
    "def confident_strategy(pred, threshold=0.8):\n",
    "    # We cast any iterable to a convenient numpy array\n",
    "    pred = np.array(pred)\n",
    "    # We are going to need to get the size twice, so let's keep it in a variable.\n",
    "    sz = len(pred)\n",
    "    # We count how many values of pred are above our threshold.\n",
    "    fakes = np.count_nonzero(pred > threshold)\n",
    "    # If more than 40 % of fakes of more than 11 fakes, \n",
    "    # we higher the probability of detecting the whole video as fake\n",
    "    ## 11 frames are detected as fakes with high probability\n",
    "    if fakes > sz // 2.5 and fakes > 11:\n",
    "        # We return the mean of all the fakes, which gives a high value\n",
    "        return np.mean(pred[pred > threshold])\n",
    "    # If 90% or more of predictions are less than 0.2, \n",
    "    # we return their mean\n",
    "    elif np.count_nonzero(pred < 0.2) > 0.9 * sz:\n",
    "        return np.mean(pred[pred < 0.2])\n",
    "    else:\n",
    "        # Else, we simply return the mean of all predictions.\n",
    "        # Thus, medium prediction confidence, between 0.2 and 0.8\n",
    "        # won't be changed by this strategy.\n",
    "        return np.mean(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image manipulation <a name=\"manip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function makes our rectangular crop of a face fit inside a rectangle of fixed sides. \n",
    "# It preserves the aspect ratio by calculating the target sizes beforehand.\n",
    "# Please refer to the readme for more explanations.\n",
    "def isotropically_resize_image(img, size):\n",
    "    # As usual, height and width are the length of the first two dimensions of our images\n",
    "    h, w = img.shape[:2]\n",
    "    # If the greatest size already has the size of our target, we can simply return the raw image\n",
    "    # as we can't do any isotropic deformation.\n",
    "    if max(w, h) == size:\n",
    "        return img\n",
    "    # We are going to calculate the goal width and height. It depends on the ratio \n",
    "    # and which side is greater. We keep the scale value to apply.\n",
    "    if w > h:\n",
    "        scale = size/w\n",
    "        h *= scale\n",
    "        w = size\n",
    "    else:\n",
    "        scale = size/h\n",
    "        w *= scale\n",
    "        h = size\n",
    "    # We choose the interpolation method according to the size of the resizing :\n",
    "    # if we want the images to get bigger (meaning target size > max(x,h)), \n",
    "    # then we use INTER_AREA interpolation, else we use INTER_CUBIC.\n",
    "    interpolation = cv2.INTER_AREA if scale >1 else cv2.INTER_CUBIC\n",
    "    # The interpolation will be done with OpenCV, and we can directly return the resulting image.\n",
    "    return cv2.resize(img, (int(w), int(h)), interpolation = interpolation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will make sure that we obtain an image of fixed size,\n",
    "# and all the original values are spread around the center of the 2D image arrays.\n",
    "# At this step, the input image should not be greater than the target size.\n",
    "def put_to_center(img, target_size):\n",
    "    # We make sure our image doesn't exceed the size limit in one or both sizes.\n",
    "    img = img[:target_size, : target_size]\n",
    "    # We initialize an empty squared array image of size input_size, with three color channels\n",
    "    # and of type unsigned 8 bits integer.\n",
    "    image = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
    "    # For both sides, the difference between the image size and target size\n",
    "    # gives us the length of the \"emptiness\". We need to divide it by two\n",
    "    # to make sure we have the same emptiness at the left and right (respectively top and bottom)\n",
    "    # of the image. In other words, we center it.\n",
    "    start_w = (target_size-img.shape[1]) // 2\n",
    "    start_h = (target_size-img.shape[0]) // 2\n",
    "    # All is remaining is slicing the empty image so that img will fit around the exact center,\n",
    "    # and we return it.\n",
    "    image[start_h:start_h + img.shape[0], start_w: start_w + img.shape[1], :] = img\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation's main pipeline <a name=\"pipeline\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are about to extract 32 frames from each video.\n",
    "frames_per_video = 32\n",
    "# This is the callable function to extract our video, but it doesn't call it right now.\n",
    "video_read_fn = lambda x: read_frames(x, num_frames=frames_per_video)\n",
    "# For the next step, we want to keep the pipeline of process to extract faces from the video.\n",
    "# The original author used a class architecture, but to stay relevant with this current projet and the notebook\n",
    "# format, we are going to use the partial function tool instead.\n",
    "# It will only need the argument video_path to be called.\n",
    "face_extractor = partial(extract_faces,  video_read_fn)\n",
    "# This value represents the size of the input of our custom predictor.\n",
    "# It was most probably got after model selection and tuning. Let's keep it.\n",
    "# If you intend to launch this notebook, and if you don't have a GPU with large memory (8 GB or more),\n",
    "# you should lower it.\n",
    "input_size = 380\n",
    "\n",
    "# Our list of videos. We keep all videos with mp4 extension, from our video folder.\n",
    "test_videos = sorted([test_video_folder+\"/\"+x for x in os.listdir(test_video_folder) if x[-4:] == \".mp4\"])\n",
    "train_videos = sorted([train_video_folder+\"/\"+x for x in os.listdir(train_video_folder) if x[-4:] == \".mp4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main porcessing loop <a name=\"loop\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per file processing <a name=\"process\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The author chose to manually store the means and stds, \n",
    "# which must be done by color chanel for pytorch image transformation.\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "# Normalize is a tensor transformation, thus we have to pass the \n",
    "# means and standard deviations as parameter, then call\n",
    "# the forward method, or call the tensor as a function.\n",
    "normalize_transform = Normalize(mean, std)\n",
    "\n",
    "# This is our final and most main-purposed function.\n",
    "# We still need it as a callable function so that we can take advantage of concurrent working.\n",
    "def process_file(i):\n",
    "    video = test_videos[i]\n",
    "    # We chose 32 frames per video. The author multiplied it by 4, so you'd better have a RTX Titan or 3090 at least.\n",
    "    batch_size = frames_per_video*4\n",
    "    try:\n",
    "        # This time, the complete function is called to extract the face.\n",
    "        # In each iteration, the function will be called at the same state, \n",
    "        # meaning the first argument we previously gave it will remain in memory.\n",
    "        faces = face_extractor(video)\n",
    "        # If no faces were found, we simply store 0.5, which is the decision boundary for classic classification.\n",
    "        if len(faces) == 0:\n",
    "            return 0.5\n",
    "        # Here, the author made a weird choice to me. I will put the block after an impossible condition test\n",
    "        # so it is never reached but we can still read it\n",
    "        # We initialize a batch of empty images, with the format: unsigned 8 bits int (0 to 255), as usual.\n",
    "        if False:\n",
    "            x = np.zeros((batch_size, input_size, input_size, 3), dtype = np.uint8)\n",
    "            n = 0\n",
    "            # We are going to process each individual face, stored in our list of convenient dictionaries.\n",
    "            for frame_data in faces:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    # To better predict with faces and have the least disparity of sizes\n",
    "                    # we apply an isotropic transformation\n",
    "                    resized_face = isotropically_resize_image(face, input_size)\n",
    "                    # Then we center the image inside a fixed size image.\n",
    "                    resized_face = put_to_center(resized_face, input_size)\n",
    "                    # At this step, all images have the exact same shape (input_size, input_size, 3).\n",
    "                    # Question\n",
    "                    # This part is a mistery. Why pass instead of leaving all the loops ? \n",
    "                    if n+1 < batch_size:\n",
    "                        x[n] = resized_face\n",
    "                        n += 1\n",
    "                    else:\n",
    "                        pass\n",
    "        # Instead of doing what the author did,\n",
    "        # we will leave both the loops when the batch is full, by using a function.\n",
    "        #######################################################################\n",
    "        # Please read and execute the next cell, then come back to this point.#\n",
    "        #######################################################################\n",
    "        x, n = resize_batch(faces, batch_size)\n",
    "\n",
    "        # We go back to the author train of thoughts.\n",
    "        # We test that we have a non-empty batch.\n",
    "        if n > 0:\n",
    "            # We can put our batch on the cuda compatible device, hopefully a powerful GPU.\n",
    "            # GPU are optimized and more performant with 32 bits floats, so we can cast it to floats.\n",
    "#             x = torch.tensor(x, devide=\"cude\").float()\n",
    "            # I personally prefer to call it like this. Please refer to the section \"Subsidiary questions\".\n",
    "            x = torch.tensor(x, device=\"cuda\", dtype=torch.float32)\n",
    "            ## Preprocess the image\n",
    "            # For the incoming pass to the models we previously selected, which deals with images data shaped\n",
    "            # in a different way, we need to permute the dimensions of our image, \n",
    "            # so that (batch, height, width, color) becomes (batch, color, height, width)\n",
    "            x = x.permute((0,3,1,2))\n",
    "            # Then, for each image of the batch :\n",
    "            for i in range(len(x)):\n",
    "                # We call normalize transform like we would do with a standard function:\n",
    "                # it takes the input tensor and returns a normalized output tensor,\n",
    "                # for each image in the batch, with the fixed mean and std values.\n",
    "                x[i] = normalize_transform(x[i] /255.)\n",
    "            # We call no_grad procedure to specify to torch \n",
    "            # that we don't want to update any tensor weight,\n",
    "            # because we finally use our model, in sequence.\n",
    "            with torch.no_grad():\n",
    "                # \n",
    "                # half() casts the tensor value to half precision (16 bits floats, from 32 bits floats).\n",
    "                # The reason to do that is it takes half less memory, and might avoid an Out of Memory Error.\n",
    "                # I still need to figure out why it provokes a RuntimeError in my setup.\n",
    "                y_pred = model(x[:n].half())\n",
    "                # squeeze() gets rid of any dimension of size 1.\n",
    "                # Here : y_pred has a shape (n,1), after squeezeing, it will have shape (n),\n",
    "                # simply a tensor array of size n. Then we call the sigmoid function\n",
    "                # which is 1 / (1 + e(-x)), also equal to e(x) / (e(x)+1)\n",
    "                y_pred = torch.sigmoid(y_pred.squeeze())\n",
    "                # bpred is simply the numpy version of the one dimensional tensor pred.\n",
    "                # It needs to be retrieved from the cpu.\n",
    "                bpred = y_pred[:n].cpu().numpy()\n",
    "                # Now we pass the predicted result to our confident strategy.\n",
    "                # This will higher the confidence of the predictions\n",
    "                # near extreme values (near 0 and 1).\n",
    "                # This is the final array of prediction. That's what we will return.\n",
    "                return confident_strategy(bpred)\n",
    "    except Exception as e:\n",
    "        # In case we have errors, we don't want to lock the other workers.\n",
    "        # We simply return the decision boundary, here again\n",
    "        traceback.print_exc() \n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can \"break\" the loop instead of processing all remaining face images of the video.\n",
    "def resize_batch(faces, batch_size):\n",
    "    x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
    "    n=0\n",
    "    for frame_data in faces:\n",
    "        for face in frame_data[\"faces\"]:\n",
    "            resized_face = isotropically_resize_image(face, input_size)\n",
    "            resized_face = put_to_center(resized_face, input_size)\n",
    "            if n + 1 < batch_size:\n",
    "                x[n] = resized_face\n",
    "                n += 1\n",
    "            else:\n",
    "                return x, n \n",
    "    return x, n "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside thought <a name=\"aside\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next and final step towards prediction involves multiple workers. To optimize this step, we actually want to know how many cpu threads are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform == 'win32':\n",
    "    n_threads = (int)(os.environ['NUMBER_OF_PROCESSORS'])\n",
    "    print(f\"Windows system with {n_threads} threads.\")\n",
    "else:\n",
    "    n_threads = (int)(os.popen('grep -c cores /proc/cpuinfo').read())\n",
    "    print(f\"System with {n_threads} threads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction <a name=\"pred\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said previously, we won't execute it. You can, but you'll get many skipped steps because the face extractor is not trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to keep track of the time taken for the prediction.\n",
    "# For this, we keep the value of the current time, and we'll substract it from the value \n",
    "# of the current time when the prediction process is completed.\n",
    "# Times are stored Unix format, which is in seconds from January, 1st 2070.\n",
    "# For example : 1st January 2020 at 00:00is the value 1577836800.\n",
    "# In 32 bits (unsigned) systems, we can relax until the max limit of July 2nd 2106 at 6:28am (UTC).\n",
    "# In 64 bits, it would reset in about 586 billion years. We should have 128 bits when this time come\n",
    "# (perhaps even 256 bits !)\n",
    "stime = time.time()\n",
    "\n",
    "# Finally we are going to make all the prediction, using a ThreadPoolExecutor\n",
    "# To spread video processing to multiple threads (using python workers).\n",
    "# We can put the number of threads we found during the precedent step.\n",
    "# Please refer to the readme for more \n",
    "with ThreadPoolExecutor(max_workers=n_threads) as ex:\n",
    "    # map is a method of the executor. It will create a worker for each video,\n",
    "    # thanks to the indexing.\n",
    "    predictions = ex.map(process_file, range(len(test_videos)))\n",
    "# We format the predictions in a convenient format.\n",
    "submission_df = pd.DataFrame({\"filename\":test_videos, \"label\": predictions})\n",
    "# We also store the results in a local csv file.\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "# The current time - the start time == elapsed time.\n",
    "print(\"Elapsed:\",time.time()-stime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsidiary questions <a name=\"subs\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does partial do ? <a name=\"partial\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Partial is a function that gets a function and a list of arguments as an argument.\n",
    "It then return a reference to the function with arguments already in memory, which is ready to be call with the remaining arguments."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here is a very basic replication of the function partial :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simili_partial(func, *args):\n",
    "    args = list(args)\n",
    "    def wrapper(*extra_args):\n",
    "        # Basically, it adds extra args to the original args list.\n",
    "        new_args = args+list(extra_args)\n",
    "        return func(*new_args)\n",
    "    # And it return a callable function, not the call to its return.\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notice how it returns a callable function, not the call of the function with the arguments !"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's say we want to compute the sum of 1, 10, 100 and one yet unknow number :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function simili_partial.<locals>.wrapper at 0x000002971D99C708>\n"
     ]
    }
   ],
   "source": [
    "partial_sum = simili_partial(sum, [1,10,100])\n",
    "print(partial_sum)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we have a function that's ready to be called whenever we want, with the remaining number as arguments."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can call this returned function with the remaining of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n"
     ]
    }
   ],
   "source": [
    "print(partial_sum(1000))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Please notice that we can keep the 'intermediay' stage without modifying it, and the partial is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10111\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "# sum([1,10,111]) is still in memory\n",
    "print(partial_sum(10000))\n",
    "print(partial_sum(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Of course, the partial class is more convenient, because it handles more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "1111\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "p = partial(sum, [1,10, 100])\n",
    "print(p())\n",
    "print(p(1000))\n",
    "print(p(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_sum = lambda a,b : a+b\n",
    "pp = partial(cust_sum, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function <lambda> at 0x0000025B3AF69D38>, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy linspace <a name=\"linspace\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "linspace(start, stop, num) returns an array [num] numbers equally spaced from [start] to [stop].\n",
    "    /?\\ num is 50 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. 10. 20. 30. 40. 50. 60. 70. 80. 90.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linspace(0,90, num=10))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By specifying the boolean endPoint argument, we can explicitely tell if the stop is included in the range (it is, by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  1.5 3. ]\n",
      "[0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linspace(start=0, stop=3, num=3, endpoint=True))\n",
    "print(np.linspace(start=0, stop=3, num=3, endpoint=False))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dtype allows us to transtype the elements of the array to another compatible one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  1.5 3. ]\n",
      "[0 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.linspace(0,3,3))\n",
    "print(np.linspace(0, 3, 3, dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Rand randint <a name=\"randint\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Randint delivers an array of size [size], with integers taken randomly from [low] to [high] (and with replacement).\n",
    "[high] is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95 14 22  7 54 20 34 43 60 58]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randint(low=0, high=100, size=10))\n",
    "print(3 in np.random.randint(low=0, high=3, size=100000))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[size] can also be a tuple, if we cant multi-dimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[81 42]\n",
      "  [42 26]\n",
      "  [36 75]]\n",
      "\n",
      " [[40 45]\n",
      "  [93 98]\n",
      "  [ 4 29]]\n",
      "\n",
      " [[66 42]\n",
      "  [54 85]\n",
      "  [71 75]]\n",
      "\n",
      " [[70 88]\n",
      "  [95 42]\n",
      "  [11 16]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randint(low=0, high=100, size=(4, 3, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy clip <a name=\"clip\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Numpy clip takes an array and limits the minimum and maximum value.\n",
    "If an element is lower than a_min, it will be replaced by a_min,\n",
    "and If an alement is higher than a_max, it will be replaced by a_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  3,  4, 10])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(np.array([1,2,3,4,15000]), a_min=2, a_max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy uint8 format <a name=\"uint8\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It can take unsigned integer values from 0 to 255, and it occupies only one byte instead of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128] [-256]\n",
      "[128] [0]\n"
     ]
    }
   ],
   "source": [
    "an_int, another_int = np.array([128]), np.array([-256])\n",
    "print(an_int, another_int)\n",
    "print(an_int.astype(np.uint8), another_int.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If we try to put a higher value, it will be as if we took the modulo 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256] [257]\n",
      "[0] [1]\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "an_int, another_int = np.array([256]), np.array([257])\n",
    "print(an_int, another_int)\n",
    "print(an_int.astype(np.uint8), another_int.astype(np.uint8))\n",
    "print(256%256, 257%256)\n",
    "del(an_int, another_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy count_nonzero <a name=\"zero\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can ask ourselves : why use np.count_nonzero(array>threshold) on an array ?\n",
    "One would simply do : (array>threshold).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.random.rand(1000)\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.97 µs ± 150 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "r1 = (test_array>threshold).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37 µs ± 69 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "r2 = np.count_nonzero(test_array>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_array>threshold).sum() == np.count_nonzero(test_array>threshold)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The reason is apparent now : it's about twice as fast !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling float() or putting the argument dtype=torch.float32 <a name=\"float\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here again, a time intensity comparison will quickly answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.64 ms ± 38.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "c = np.random.randint(256, size=(16, 96, 128, 3))\n",
    "ct=torch.tensor(c, device=\"cuda\", dtype=torch.float32)\n",
    "try:\n",
    "    del(ct)\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.41 ms ± 48.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "c = np.random.randint(256, size=(16, 96, 128, 3))\n",
    "ct=torch.tensor(c, device=\"cuda\").float()\n",
    "try:\n",
    "    del(ct)\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the top](#start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
